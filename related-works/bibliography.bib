@INPROCEEDINGS{10793163,
    author={Yang, Zeyu and Adamek, Karel and Armour, Wesley},
    booktitle={SC24: International Conference for High Performance Computing, Networking, Storage and Analysis},
    title={Accurate and Convenient Energy Measurements for GPUs: A Detailed Study of NVIDIA GPUâ€™s Built-In Power Sensor},
    year={2024},
    volume={},
    number={},
    pages={1-17},
    keywords={Meters;Accuracy;Power measurement;Power demand;Runtime;Prevention and mitigation;Measurement standards;Energy measurement;Graphics processing units;Energy efficient computing;High performance computing;Green computing;Energy consumption;Energy measurement;Power measurement},
    doi={10.1109/SC41406.2024.00028}}
@ARTICLE{10975210,
    author={Jaradat, Ghadeer A. and Tolba, Mohammed F. and Alsuhli, Ghada and Saleh, Hani and Al-Qutayri, Mahmoud and Stouraitis, Thanos},
    journal={IEEE Transactions on Artificial Intelligence},
    title={Efficient Transformer Inference Through Hybrid Dynamic Pruning},
    year={2025},
    volume={},
    number={},
    pages={1-14},
    keywords={Transformers;Computational modeling;Computational efficiency;Hardware;Attention mechanisms;Memory management;Heuristic algorithms;Head;Artificial intelligence;Accuracy;Hardware acceleration;dynamic pruning;approximation;self attention;transforme},
    doi={10.1109/TAI.2025.3563144}}
@misc{tambe2021edgebertsentencelevelenergyoptimizations,
      title={EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
      author={Thierry Tambe and Coleman Hooper and Lillian Pentecost and Tianyu Jia and En-Yu Yang and Marco Donato and Victor Sanh and Paul N. Whatmough and Alexander M. Rush and David Brooks and Gu-Yeon Wei},
      year={2021},
      eprint={2011.14203},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2011.14203},
}
@INPROCEEDINGS{9251854,
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  title={GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference},
  year={2020},
  volume={},
  number={},
  pages={811-824},
  keywords={Quantization (signal);Tensors;Computational modeling;Memory management;Hardware;Energy efficiency;Task analysis},
  doi={10.1109/MICRO50266.2020.00071}}

@misc{hunter2021sparsitiesbetteroneunlocking,
      title={Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks},
      author={Kevin Lee Hunter and Lawrence Spracklen and Subutai Ahmad},
      year={2021},
      eprint={2112.13896},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.13896},
}

@inproceedings{10.5555/3524938.3525451,
author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan},
title = {Inducing and exploiting activation sparsity for fast neural network inference},
year = {2020},
publisher = {JMLR.org},
abstract = {Optimizing deep neural networks for inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions. In this paper, we present an analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with relatively low retraining cost.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {513},
numpages = {11},
series = {ICML'20}
}

